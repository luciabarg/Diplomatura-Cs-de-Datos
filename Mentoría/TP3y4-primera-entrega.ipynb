{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ul-8vdGw4NJ"
   },
   "source": [
    "<center>\n",
    "<h4>Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación</h4>\n",
    "<h3>Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</h3>\n",
    " <h2>Mentoría: Clasificación diagnóstica de mamografías </h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "190G7l7fKXoj"
   },
   "source": [
    "<h3> Práctico III y IV -  Aprendizaje Supervisado <h3>\n",
    "<h4>Integrantes: Mario Agustín Sgró, Lucía Benítez y Carolina Díaz <h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo y alcance:** \n",
    "  Conocimiento y práctica sobre técnicas básicas de aprendizaje automático discriminando por tipo de variable a predecir. Estudiar aumentación de datos polinomial asociada a cambio de modelo. Continuación del práctico anterior. Pueden entregarse juntos presentando únicamente este práctico, pues incluye lo pedido en el anterior.\n",
    "\n",
    "**Método:** \n",
    "Regresión lineal y polinomial, clasificación Regulación.\n",
    "\n",
    "\n",
    "**Estructura del informe:** \n",
    "Presentar en un archivo jupyter notebook con la resolución detallada de las siguientes consignas:\n",
    "\n",
    "1- Separar la base en dos conjuntos: Train (con el 70 % de los datos) y Test con el 30%) \n",
    " \n",
    "2- Elegir 2 variables/características/features numéricas de la base de datos, una proveniente del grupo de las Haralick y otra asociada a la Dimensión Fractal:\n",
    "\n",
    "            a) Considerando una como regresora y la otra como objetivo o variable/característica a predecir, realizar una regresión lineal y varias polinomiales (hasta grado 5) para evaluar el grado del polinomio que mejor se ajusta a las predicciones (Kernel trick). Realizar la búsqueda utilizando validación cruzada  en el conjunto Train, con RMSE o análoga como medida de calidad de ajuste.\n",
    "            b) Agregar un parámetro de regulación, convirtiendo a esta búsqueda en búsqueda por grilla.\n",
    "            c) Una vez definidos los hiperparámetros (encontrados en la búsqueda del item anterior. Evaluar el modelo ajustado utilizando el conjunto Test, con la medida (o las medidas) que crea adecuada.\n",
    "\n",
    "3- Definir una nueva variable categórica (binaria) con clase 1 como clase Ay B y clase 2 como clase Cy D de densidad mamaria. Y reducir la base considerando además sólo dos de todas las variables numéricas, es decir, la nueva base tendrá sólo tres variables. Pero sigue estando dividida en los conjuntos Train y Test.\n",
    "Implementar el algoritmo de perceptrón en el conjunto Train para clasificación binaria: clase 1 vs clase 2. \n",
    "Evaluar con la medidas que crea adecuadas y graficar los resultados en tres instancias de aprendizaje (con un tercio de la base, con 2 tercios y con la base Train). \n",
    "Evaluar cada instancia en el conjunto Test y comparar los resultados con los resultados sobre Train.\n",
    "Comentar si se presenta sobre ajuste, sesgo o ninguno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ws1LOM6rw4_V"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "#import chardet\n",
    "import string\n",
    "plt.rcParams['figure.figsize'] = (5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4QmUse8D_dT"
   },
   "outputs": [],
   "source": [
    "if sns.__version__ != '0.9.0':\n",
    "    print('Atención! utilizamos seaborn versión 0.9.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYwCKRgyAfO7"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "#pd.options.display.max_colwidth = 100\n",
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hta6KY8LD_dX"
   },
   "source": [
    "# Carga del DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('datos/Datos_Mamografias.csv',sep=',',index_col=['Imagen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo analizado en el práctico anterior, sabemos que la base de datos posee filas con datos faltantes, por lo que en lo siguiente, ocuparemos una base de datos ya limpia, con sus filas sin datos eliminadas y con la imputación de datos faltantes por medio de vecinos mas cercanos (knn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtSUUZGhD_dX"
   },
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('datos/KnnFilled_data.csv',sep=',',index_col=['Imagen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uavh9KhOHd-"
   },
   "source": [
    "## Descripción de la base de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RulVSer_AfPA",
    "outputId": "e7e03388-391a-485d-c8f6-9a91cdc06c42"
   },
   "outputs": [],
   "source": [
    "dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dju0Zdl1AfPJ",
    "outputId": "b72c01b0-a55c-4778-8a0b-9af805341301",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"El data set consiste de \",dataframe.shape[0],\" imagenes, con \",dataframe.shape[1],\" registros cada una.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['Dcm_6'] = dataframe['Dcm_6'].astype('category')\n",
    "dataframe['Dcm_13'] = dataframe['Dcm_13'].astype('category')\n",
    "dataframe['Dcm_11'] = dataframe['Dcm_11'].astype('category')\n",
    "dataframe['Dcm_21'] = dataframe['Dcm_21'].astype('category')\n",
    "dataframe['Dcm_22'] = dataframe['Dcm_22'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el presente práctico se propone hacer la división de los _targets_ en dos grandes clases, una que contiene a los datos con la variable \"ACR\" igual a _a_ o _b_ y la otra con los datos con dicha variable igual a _c_ o _d_. Por ello creamos una nueva variable target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"Class\"] = dataframe[\"ACR\"].copy()\n",
    "\n",
    "dataframe[\"Class\"].replace({\"a\":0,\"b\":0,\"c\":1,\"d\":1},inplace=True)\n",
    "dataframe['Class'] = dataframe['Class'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NoRMe8bRD_dw"
   },
   "source": [
    "## Tipos de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vc6Mtw3gD_dw",
    "outputId": "67322704-668e-4a36-a78f-be16332b10d1"
   },
   "outputs": [],
   "source": [
    "CatFeat = [x for x in dataframe.columns if dataframe[x].dtype == 'object']\n",
    "NumFeat = [x for x in dataframe.columns if dataframe[x].dtype == 'float64']\n",
    "IntFeat = [x for x in dataframe.columns if dataframe[x].dtype == 'int64']\n",
    "\n",
    "print('Cantidad' + str('\\n')+\n",
    "      '   variables numéricas     '+ str(len(NumFeat)) + str('\\n') +\n",
    "      '   variables enteras     '+ str(len(IntFeat)) + str('\\n')\n",
    "      + '   variables discretas  ' + str(len(CatFeat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in CatFeat:\n",
    "    dataframe[cat] = dataframe[cat].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CatFeat = [x for x in dataframe.columns if dataframe[x].dtype.name == 'category']\n",
    "NumFeat = [x for x in dataframe.columns if dataframe[x].dtype.name != 'category']\n",
    "\n",
    "print('Cantidad' + str('\\n')+\n",
    "      '   variables numéricas     '+ str(len(NumFeat)) + str('\\n')\n",
    "      + '   variables discretas  ' + str(len(CatFeat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos las variables categóricas con palabras en numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataframe.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in CatFeat:\n",
    "    data[cat] = data[cat].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Dividimos la muestra en un conjunto de training y uno de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"ACR\",\"Class\"],axis=1),data.Class,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map = {feature: idx for idx, feature in enumerate(X_train.columns)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkeamos el desbalance de clases 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = np.bincount(y_test)\n",
    "print(bal[1]/bal[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las clases están bien balanceadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos algunas herramientas que utilizaremos luego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Regresión lineal y polinomial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomamos 2 variables numéricas de la base de datos, una de las Haralick (Haralick_46)y otra correspondiente a la Dimensión Fractal (DF16):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataframe,hue='ACR',x_vars=['Haralick_46','DF16'],y_vars=['Haralick_46','DF16'])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Tomamos a la característica Haralick como regresora y la DF como variable objetivo: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataframe.Haralick_46.values\n",
    "y = dataframe.DF16.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "grados = np.arange(1,6)\n",
    "for i in range(len(grados)):\n",
    "    ax = plt.subplot(1, len(grados), i + 1)\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=grados[i],include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(x[:, np.newaxis], y)\n",
    "\n",
    "    scores = cross_val_score(pipeline, x[:, np.newaxis],y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "    X_test = np.linspace(x.min(), x.max(), 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Modelo\")\n",
    "    plt.scatter(x, y, edgecolor='b', s=20, label=\"Datos\")\n",
    "    plt.xlabel(dataframe.Haralick_46.name)\n",
    "    plt.ylabel(dataframe.DF16.name)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Grado {}\\nMSE = {:.2e}(+/- {:.2e})\".format(grados[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que la relación entre ellas no es lineal y que el polinomio que mejor se ajusta es el de 3er grado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con regularización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_degree = 3\n",
    "\n",
    "for alpha in np.logspace(-6,0.0,10):\n",
    "\n",
    "    poly_features = PolynomialFeatures(polynomial_degree)\n",
    "    poly_features.fit(x[:, np.newaxis])\n",
    "    X_poly_train = poly_features.transform(x[:, np.newaxis])\n",
    "\n",
    "    model = Ridge(alpha=alpha,normalize=True,fit_intercept=True)\n",
    "    model.fit(X_poly_train, y)\n",
    "\n",
    "    print('valor de alpha%.2e: Media del error cuadrado para entrenamiento: %.2f' % (alpha,mean_squared_error(y, model.predict(X_poly_train))))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_degree = 3\n",
    "\n",
    "for alpha in np.logspace(-6,0.0,10):\n",
    "\n",
    "    poly_features = PolynomialFeatures(polynomial_degree)\n",
    "    poly_features.fit(x[:, np.newaxis])\n",
    "    X_poly_train = poly_features.transform(x[:, np.newaxis])\n",
    "\n",
    "    model = Lasso(alpha=alpha,normalize=True,fit_intercept=True)\n",
    "    model.fit(X_poly_train, y)\n",
    "\n",
    "    print('%.2e: Media del error cuadrado para entrenamiento: %.2f' % (alpha,mean_squared_error(y, model.predict(X_poly_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos casos, tanto en Ridge como en Lasso, alpha=2.15e-05 puede tomarse como parámetro de regularización antes de que aumente el error cuadrático medio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Validación cruzada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_degree = 3\n",
    "alpha = 2.15e-5\n",
    "\n",
    "poly_features = PolynomialFeatures(polynomial_degree)\n",
    "poly_features.fit(x[:, np.newaxis])\n",
    "X_poly_train = poly_features.transform(x[:, np.newaxis])\n",
    "\n",
    "model = Lasso(alpha=alpha,normalize=True,fit_intercept=True)\n",
    "model.fit(X_poly_train, y)\n",
    "\n",
    "X_test = np.linspace(x.min(), x.max(), 100).reshape(-1,1)\n",
    "X_poly_test = poly_features.transform(X_test)\n",
    "\n",
    "scores = cross_val_score(model, X_poly_train,y,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "\n",
    "plt.plot(X_test, model.predict(X_poly_test), label=\"Model\")\n",
    "plt.scatter(x, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "plt.xlabel(dataframe.Haralick_46.name)\n",
    "plt.ylabel(dataframe.DF16.name)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Grado {}\\nMSE = {:.1f}\".format(polynomial_degree, mean_squared_error(y, model.predict(X_poly_train))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementar el algoritmo de perceptrón para clasificación binaria: consideranco clase 1 como clase Ay B y a clase 2 como clase Cy D de densidad mamaria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop([\"ACR\",\"Class\"],axis=1),data.Class,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el algoritmo de perceptrón debemos tener normalizados los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train.copy()\n",
    "X_test_norm = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Min = X_train_norm.min()\n",
    "Max = X_train_norm.max()\n",
    "Mean = X_train_norm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_norm.shape,X_test_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = (X_train_norm - Mean)/(Max - Min)\n",
    "X_test_norm = (X_test_norm - Mean)/(Max - Min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consideramos los valores por defecto del estimador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(tol=1.e-7,random_state=167,fit_intercept=True,max_iter=10000,validation_fraction=0.3)\n",
    "clf.fit(X_train_norm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buscamos los mejores parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(random_state=42,fit_intercept=True,max_iter=10000,tol=1.0e-7,validation_fraction=0.3)\n",
    "\n",
    "#presort True o False da lo mismo\n",
    "\n",
    "alphas = np.logspace(-8,0,9)\n",
    "etas = np.logspace(-8,0,9)\n",
    "\n",
    "\n",
    "param_grid = {\"alpha\":alphas,\n",
    "              \"eta0\": etas,\n",
    "              \"penalty\": (\"l1\",\"l2\",None,\"elasticnet\")\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(clf,param_grid=param_grid,cv=5,scoring=\"f1\",iid=False,n_jobs=8)\n",
    "start = time()\n",
    "grid.fit(X_train_norm, y_train)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid.cv_results_['params'])))\n",
    "report(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test_norm,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(random_state=42,alpha=0.01,penalty='l1',fit_intercept=True,max_iter=10000,tol=1.0e-7)\n",
    "clf = clf.fit(X_train_norm,y_train)\n",
    "model = SelectFromModel(clf,prefit=True,threshold=-np.inf,max_features=20)\n",
    "X_new = model.transform(X_train_norm)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = model.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm.columns[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui no nos seleccionó ninguna Haralick :(.. Probaremos algo más a fuerza bruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuerza bruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_features(feature1,feature2,penalty='l1',alpha = 0.01,max_iter = 10000):\n",
    "    X_train_feature = X_train_norm.loc[:, [feature1, feature2]]\n",
    "    X_test_feature = X_test_norm.loc[:, [feature1, feature2]]\n",
    "    \n",
    "    model = Perceptron(penalty=penalty, alpha=alpha, max_iter=max_iter,tol=1.e-7)\n",
    "    model.fit(X_train_feature, y_train)\n",
    "\n",
    "    return(accuracy_score(y_train, model.predict(X_train_feature)),accuracy_score(y_test, model.predict(X_test_feature)))    \n",
    "\n",
    "_myf1 = ''\n",
    "_myf2 = ''\n",
    "\n",
    "for _p in ['l1','l2','elasticnet']:\n",
    "    score = 0.0\n",
    "    for feature1 in NumFeat:\n",
    "        for feature2 in NumFeat:\n",
    "            if feature1 == feature2:\n",
    "                continue\n",
    "    \n",
    "            _x, _y = best_features(feature1,feature2,penalty=_p,alpha=10.0)\n",
    "            if _x > score:\n",
    "                _myf1 = feature1\n",
    "                _myf2 = feature2\n",
    "                score = _x\n",
    "    print(_p,\"feature1: \",_myf1,\"feature2: \",_myf2,score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrón sobre características seleccionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from ml.visualization import classifier_boundary\n",
    "from matplotlib.colors import ListedColormap\n",
    "from utils import plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos dos atributo de los listados en el apartado anterior, uno para el eje x y otro para el eje y\n",
    "x_feature = 'Haralick_48'\n",
    "y_feature = 'DFb31'\n",
    "\n",
    "X_train_feature = X_train_norm.loc[:, [x_feature, y_feature]]\n",
    "X_test_feature = X_test_norm.loc[:, [x_feature, y_feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceptron(max_iter=10000,tol=1.e-7,alpha=0.01, eta0=1e-06, penalty='l2')\n",
    "model.fit(X_train_feature, y_train)\n",
    "model.score(X_test_feature,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "xx, yy, Z = classifier_boundary(np.r_[X_train_feature, X_test_feature], model,h=0.01)\n",
    "\n",
    "\n",
    "cmap_dots = ListedColormap(['tomato', 'dodgerblue'])\n",
    "cmap_back = ListedColormap(['lightcoral', 'skyblue'])\n",
    "\n",
    "# Conjunto de entrenamiento\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_back)\n",
    "plt.scatter(X_train_feature.iloc[:, 0], X_train_feature.iloc[:, 1], c=y_train, cmap=cmap_dots, edgecolor=None, s=1)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Conjunto de Entrenamiento\")\n",
    "\n",
    "# Conjunto de validación\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_back)\n",
    "plt.scatter(X_test_feature.iloc[:, 0], X_test_feature.iloc[:, 1], c=y_test, cmap=cmap_dots, edgecolor=None, s=1)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Conjunto de Validación\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buscamos mejores parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Perceptron(random_state=42,fit_intercept=True,max_iter=10000,tol=1.0e-7,validation_fraction=0.3)\n",
    "\n",
    "#presort True o False da lo mismo\n",
    "\n",
    "alphas = np.logspace(-12,0,13)\n",
    "etas = np.logspace(-12,0,13)\n",
    "\n",
    "\n",
    "param_grid = {\"alpha\":alphas,\n",
    "              \"eta0\": etas,\n",
    "              \"penalty\": (\"l1\",\"l2\",None,\"elasticnet\")\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid,cv=5,scoring=\"f1\", iid=False,n_jobs=4)\n",
    "start = time()\n",
    "grid.fit(X_train_feature, y_train)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid.cv_results_['params'])))\n",
    "report(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene que los mejores párametros para el algoritmo de perceptrón son:\n",
    "\n",
    "    alpha = 1.e-5\n",
    "    eta = 1.e-12\n",
    "    penalty = l1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluar y graficar los resultados en tres instancias de aprendizaje utilizando dos de todas las variables numéricas. Con imax, se define distintas porciones de train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2,figsize=(10,15))\n",
    "\n",
    "cmap_dots = ListedColormap(['tomato', 'dodgerblue'])\n",
    "cmap_back = ListedColormap(['lightcoral', 'skyblue'])\n",
    "  \n",
    "for i in range(3):        \n",
    "\n",
    "    _imax = X_train_feature.shape[0]//3*(i+1)\n",
    "    print(_imax)\n",
    "    \n",
    "    _X = X_train_feature.iloc[0:_imax,:]\n",
    "    _Y = y_train.iloc[0:_imax]\n",
    "\n",
    "    model = Perceptron(max_iter=10000,tol=1.0e-7,alpha=1.0e-5, eta0=1.0e-12, penalty='l1',random_state=617)\n",
    "    model.fit(_X, _Y)\n",
    "      \n",
    "    print(model.n_iter_,model.coef_,model.score(X_test_feature,y_test))\n",
    "        \n",
    "    xx, yy, Z = classifier_boundary(np.r_[_X, X_test_feature], model,h=0.01)\n",
    "\n",
    "    # Conjunto de entrenamiento\n",
    "    axs[i,0].pcolormesh(xx, yy, Z, cmap=cmap_back)\n",
    "    axs[i,0].scatter(_X.iloc[:, 0], _X.iloc[:, 1], c=_Y, cmap=cmap_dots, edgecolor=None, s=1)\n",
    "    axs[i,0].set_xlim(-0.2,0.2)\n",
    "    axs[i,0].set_ylim(-0.2,0.2)\n",
    "        \n",
    "    if i == 0:\n",
    "        axs[i,0].set_title(\"Conjunto de Entrenamiento\")\n",
    "\n",
    "    # Conjunto de validación\n",
    "    axs[i,1].pcolormesh(xx, yy, Z, cmap=cmap_back)\n",
    "    axs[i,1].scatter(X_test_feature.iloc[:, 0], X_test_feature.iloc[:, 1], c=y_test, cmap=cmap_dots, edgecolor=None, s=1)\n",
    "    axs[i,1].set_xlim(-0.2,0.2)\n",
    "    axs[i,1].set_ylim(-0.2,0.2)\n",
    "        \n",
    "    if i == 0:\n",
    "        axs[i,1].set_title(\"Conjunto de Validación\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probando otros clasificadores: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decission Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DTC(random_state=167)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DTC(random_state=42,splitter=\"best\",presort=True,criterion=\"entropy\")\n",
    "\n",
    "#presort True o False da lo mismo\n",
    "\n",
    "param_grid = {#\"criterion\":(\"gini\",\"entropy\"),\n",
    "              \"max_features\": (50,55,60,65,70),\n",
    "              \"min_samples_split\": (2,3,4)\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=5,scoring=\"f1\", iid=False,n_jobs=4)\n",
    "start = time()\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid.cv_results_['params'])))\n",
    "report(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bagging.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un score de .75, aun sin optimizar. Veamos si podemos optimizar un poco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(KNeighborsClassifier(), random_state=42)\n",
    "\n",
    "param_grid = {\"max_samples\": (0.5,0.8),\n",
    "              \"max_features\": (0.01,0.05),\n",
    "              \"bootstrap\": (True, False),\n",
    "              \"n_estimators\": (50,60)\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False,n_jobs=4)\n",
    "start = time()\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid.cv_results_['params'])))\n",
    "report(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RFC(random_state=169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos un score de .82, aun sin optimizar. Veamos si podemos optimizar un poco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RFC(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "              \"max_features\": (\"sqrt\",None),\n",
    "              \"bootstrap\": (True, False),\n",
    "              \"n_estimators\": (60,100)\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False,n_jobs=4,scoring=\"f1\")\n",
    "start = time()\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid.cv_results_['params'])))\n",
    "report(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred, classes = np.array([0,1,2]), \n",
    "                      normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Correcc_Practico2_entrega.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
